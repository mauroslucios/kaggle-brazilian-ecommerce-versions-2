


# Creating Session and import libs
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, count

# Create a Spark Session
spark = SparkSession.builder.appName("Order olist").getOrCreate()





# Load the CSV files into PySpark DataFrames
orders = spark.read.csv('data/olist_orders_dataset.csv', header=True, inferSchema=True)

# Convert order_purchase_timestamp to date
orders = orders.withColumn('order_date', to_date('order_purchase_timestamp'))

# Group by order_date and count the number of orders
order_distribution = orders.groupBy('order_date').agg(count('order_id').alias('order_count'))

# Show the result
order_distribution.show()





# Load the CSV files into PySpark DataFrames
order_itens = spark.read.csv('data/olist_order_items_dataset.csv', header=True, inferSchema=True)
reviews = spark.read.csv('data/olist_order_reviews_dataset.csv', header=True, inferSchema=True)

# Join order_items and reviews on order_id
joined_df = order_items.join(reviews, order_items.order_id == reviews.order_id, 'left')

# Select relevant columns
columns = ['price','freight_value','review_score']
correlation_df = joined_df.select(columns)

# calculation correlation between price, freight_value, and review_score
correlation_matrix = correlation_df.toPandas().corr()

# Show the correlation matrix
print(correlation_matrix)





import pandas as pd
import folium

# Load the CSV files into Pandas DataFrames
customers = pd.read_csv('data/olist_customers_dataset.csv')
geolocation = pd.read.csv('data/olist_geolocation_dataset.csv')



